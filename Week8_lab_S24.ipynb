{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643b70e1",
   "metadata": {},
   "source": [
    "# Lab Week 8\n",
    "\n",
    "#### In the last lab, we:\n",
    "- merged our paired end reads that overlapped using `bbmerge`\n",
    "- mapped our reads to the reference genome using `bwa`\n",
    "- converted to bam files, filtered out low quality matches, and sorted for downstream processing with `samtools`\n",
    "\n",
    "We did all this on small, downsampled fastq files to make the programs run faster so we could see each of the individual steps. Over the weekend, I ran the full sequencing fastq files through this process to map the hundreds of millions of reads contained in each file to the reference genome, a process that took many hours and many computer cores for each file. Today we will continue to process these files to identify SNPs from the sequencing reads. Ultimately this will allow us to count the reads supporting the reference and alternate alleles to estimate allele frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e41f3",
   "metadata": {},
   "source": [
    "1) To get started, Log into spydur from the terminal tab with your net id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b95a4-546b-4715-aba9-e9e213ca34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh YOURNETID@spydur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85876c1a",
   "metadata": {},
   "source": [
    "2) Next we are going to start a job on the erickson partition of spydur so that you can use more memory and computer cores. Run the code below, which is the same as what we used last week to activate your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "srun --pty -t 3:00:00 --mem=40G --partition erickson --ntasks-per-node=3 bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0493d",
   "metadata": {},
   "source": [
    "3) After this has started, navigate to your personal folder in the `shared_perickso` directory and make a new folder for `Lab8` this week (remember, no spaces in file or folder names, this causes lots of problems in Unix! And make sure the folder is in your personal folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a44751",
   "metadata": {},
   "source": [
    "4) Just like last week: create a variable called `$WD` that will serve as a shortcut to your Lab8 folder and confirm it saved with `echo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WD=\n",
    "echo $WD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da9799",
   "metadata": {},
   "source": [
    "5) Next, change directories to `~/shared_perickso/bams` (remember the tilde, ~, is a shortcut to /home/YOURNETID), and find the names of the final`.bam` file for your two sequencing samples. These files are rather large, so instead of copying them to your directory, we are going to put in links (like a hyperlink) using the function `ln`. Modify the code below to link your samples to your working directory for this week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5404fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln <filename> $WD\n",
    "ln <filename> $WD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca26211",
   "metadata": {},
   "source": [
    "6) Navigate back to your Lab8 folder (you can use `cd $WD` as a shortcut) and make sure the bam files are there using `ls`. It is really important that you are working in this folder for the rest of the lab! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30cecf",
   "metadata": {},
   "source": [
    "7) Next we need to make a text file that records full paths of both of our working bam files that are in our `Lab8` folder. The program we will run needs to read this file in. Below, the code uses asterisk wildcard to represent any letter or number coming after \"ZP\". In other words `ZP*.bam` will match any filename that starts with `ZP`. So this command lists all the files in your folder that start with ZP and end with .bam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996536af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $WD/ZP*.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31baa768",
   "metadata": {},
   "source": [
    "Now use this same trick to save these full file names to a file called `sample_files.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c314bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $WD/ZP*.bam > sample_files.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6628c59",
   "metadata": {},
   "source": [
    "**Question: Look at the `sample_files.txt` that you just made above using `less`. What did that code that you just ran do?**\n",
    "\n",
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ed800",
   "metadata": {},
   "source": [
    "8) Now we need to make a second file that contains just the names of our two samples on separate lines. We'll be using the UNIX tool `echo`, which basically means \"print\". We will tell it to print our two file names with a line break in between them. The special character `\\n` says to start a new line. Look at the command below and edit it for your file names. Inside the quotes, replace with the two names of your samples but leave the `\\n`, for example `\"ZP_1\\nZP_2\"`, but subbing your sample names for the 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3248dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "echo -e \"first_sample\\nsecond_sample\" > sample_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330edb0",
   "metadata": {},
   "source": [
    "9) Look at your `sample_names.txt` file with `less` to make sure it did what you wanted it to (each sample names should be on a separate line with no extra characters. If this file is not correct, the following steps will not work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5907812f",
   "metadata": {},
   "source": [
    "10) We need to create an index file for each of our mapped bam files; this is like the index of a book, which tells you where to look for certain information in the book. The program we used last week, `samtools` can do this. The index file helps the computer find reads in different parts of the genome more quickly. Edit the code to generate an index file for each of your mapping bam files. This will take a couple minutes. Confirm that two files ending in `.bai` were created. They are binary, so we can't look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72061078",
   "metadata": {},
   "outputs": [],
   "source": [
    "samtools index <bam_file_1>\n",
    "samtools index <bam_file_2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd40483-b06c-4431-a004-28daecf96771",
   "metadata": {
    "tags": []
   },
   "source": [
    "11) For our first analysis we are going to find out how much sequencing data we actually successfully mapped to the genome in the bam files. The term \"depth\" refers to how many sequencing reads cover or map to any given position in the genome. So if 100 sequencing reads overlap with a particular location, that location would have  a depth of 100. Another location might have slightly more reads overlapping it for a depth of 105.\n",
    "\n",
    "The program we'll use to calculate depth is a function of the samtools program we used last week. The general format is `samtools depth <input_bam_file> > <output.txt>`\n",
    "\n",
    "Edit the general code below to update with your bam file name (the one sample you worked with last week; your partner will do the other), and name the output file \"samplename_depth.txt\" (but update with the actual name of your sample, ZP_#!). You and your partner can each calculate depth for the file you were working with last week. This calculation might take a couple of minutes, but you can start reading ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\n",
    "output=\n",
    "samtools depth $input > $output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbd299-a35c-463f-9ef2-17b591dab103",
   "metadata": {},
   "source": [
    "**Question:  The file you just made has one line for every position in the genome (so ~150,000,000 lines) and the third column  contains the number of sequencing reads mapped to each position. Look at the output file you just created with `less`. What is the sequencing depth for the first position in the genome? What happens to the depth as you move away from the end of the chromosome?**\n",
    "\n",
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a1a76",
   "metadata": {},
   "source": [
    "12) We can calculate the average sequencing depth for each scaffold using something called a **\"for loop\"** in unix. A **for loop** takes a variable and repeats calculations over a range of values for that variable. In this case, the code is taking a variable called `i` and executing the code for `i=1`, `i=2`...`i=5`. The code below uses a program called `awk` to calculate an average of all the numbers found in column 3 of the depth file.  Copy this whole block of code and run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {1..5}; do\n",
    "echo \"calculating depth for scaffold \" $i\n",
    "   y=`grep Scaffold_${i} $output | awk '{sum+=$3} END { print sum/NR}'`\n",
    "    echo \"the average depth of scaffold\" $i \"is\" $y\n",
    "    done  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7a6c9",
   "metadata": {},
   "source": [
    "**Question: why did the code loop through the numbers 1 to 5? Hint: where does the variable `{i}` show up in the code and what is that line doing?**\n",
    "\n",
    "**Answer here**\n",
    "\n",
    "**Question: Which scaffold has the lowest seuqencing depth? Can you think of a reason one chromosome might have lower sequencing depth than the others? Hint: think about what chromosome is not present equally in all individuals in most species, and how that would be reflected when pooling many individuals together.**\n",
    "\n",
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3704bb8-b8b5-4a92-a521-f847f0879d65",
   "metadata": {},
   "source": [
    "13) Next we are going to generate what is called a \"pileup file\". This file takes all the mapping information from the bam files, including positions where there were mismatches the the reference genome, and summarizes it for every position in the genome. We are just going to create it for a small region of the genome (Scaffold_1, positions 10,000,000-11,000,000) to speed up our future analysis. Complete the code below, assigning  a name for your pileup file and start running it, then answer the questions below while it runs. \n",
    "\n",
    "The first line is telling the program where to find the reference genome and needs to be completed with the full location of our reference genome from the past weeks. The second line defines the name of the pileup output file. **Make sure you change the name of the `$pileup` file to match your sample names; it should be named `sample1_sample2.pileup`, where sample1 and sample2 are the two ZP samples.** The third line uses `samtools` to generate the pileup file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE=\n",
    "pileup=$WD/\n",
    "\n",
    "#make mpileup file of your data-\n",
    "samtools mpileup -r Scaffold_1:10000000-11000000 -B -f $REFERENCE --bam-list sample_files.txt > $pileup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0962d",
   "metadata": {},
   "source": [
    "14) While the program is running, go to this website: http://www.htslib.org/doc/samtools-mpileup.html and read about the pileup format to answer the following questions:\n",
    "\n",
    "**Question:  Approximately how many lines will you expect to have in your pileup file given the region of the genome that we are looking at?**\n",
    "\n",
    "**Answer here** \n",
    "\n",
    "**Question: How are bases that match the reference base indicated in the pileup file? How are SNPs indicated?**\n",
    "\n",
    "**Answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ff1f8",
   "metadata": {},
   "source": [
    "15) After your code has run, look at your pileup file using `less -S` and answer the following question:\n",
    "\n",
    "**Question: Given what you learned above, where do you see the first SNP and how do you know?** \n",
    "\n",
    "**Answer here** \n",
    "\n",
    "**Question: Now count the number of lines in your pileup file with `wc -l <filename>`. Does it approximately meet your expectation?**\n",
    "\n",
    "**Answer here** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d8ac4-f4f2-4633-9977-5de839451e42",
   "metadata": {},
   "source": [
    "16) Now we will actually identify SNPs and indels in this small region of the genome. The code below uses a program called `Varscan` to identify the locations of variants using the pileup file. We set filters so that it requires variants to have a coverage of at least 20 reads and a minimum allele frequency of 0.05. The p-value filter refers to a statistical probability that the SNP is real. The SNPs and indels are called with two separate commands `mpileup2snp` and `mpileup2indel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b670710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure you are in your working directory\n",
    "\n",
    "cd $WD\n",
    "\n",
    "#tell the computer how much memory to use\n",
    "\n",
    "JAVAMEM=40g\n",
    "\n",
    "#call SNPs\n",
    "java -Xmx$JAVAMEM -jar /usr/local/sw/varscan/VarScan.v2.3.9.jar mpileup2snp $pileup \\\n",
    "  --output-vcf 1 \\\n",
    "  --min-coverage 20 \\\n",
    "  --min-var-freq 0.05 \\\n",
    "  --p-value 0.1 \\\n",
    "  --vcf-sample-list sample_names.txt > raw_snps.vcf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537c0c7-82bb-4512-8ace-fc191bf8c2a3",
   "metadata": {},
   "source": [
    "**Question: Why might we want to only find SNPs with a depth of at least 20?**  \n",
    "\n",
    "**Answer here** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb4ffd",
   "metadata": {},
   "source": [
    "**Question: While the code is running, go to https://www.ebi.ac.uk/training/online/courses/human-genetic-variation-introduction/variant-identification-and-analysis/understanding-vcf-format/ to learn about the output file, vcf files. Complete the activity and take a screenshot when you are done. Paste your screenshot here. It will just show up as a link to a jpg file but will be inserted when you export to PDF at the end**\n",
    "\n",
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6433181",
   "metadata": {},
   "source": [
    "**Question: Read the output that printed to the terminal when your code was done. How many SNPs did it identify?**\n",
    "\n",
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb136ed",
   "metadata": {},
   "source": [
    "17) The first several lines of the vcf file begin with # and have important information about the data that can be found in the file. Use `less -S` to look at your raw SNP calls vcf file to answer the following questions. They will be very difficult to read if you don't add the -S to make things print in nice columns. If you are stuck on these questions, please talk to me because I want to be sure you understand this information!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6f69d",
   "metadata": {},
   "source": [
    "The vcf will store a lot of information about each variant in each sample. The key to decoding this information is provided in the header lines that start with ##FORMAT=. In this line it will say ID=, which gives the name of a particular piece of data. Then it will give a description of that data. So for example, the first ##FORMAT has \"ID=GT\" and the description is \"genotype\". This means that any data in the vcf labeled GT is giving you a genotype.\n",
    "\n",
    "**Question: Look at the #FORMAT lines of your vcf. What are the names used in the format column to indicate the number of reference reads and alternate reads?**\n",
    "\n",
    "**Answer here**\n",
    "\n",
    "\n",
    "**Question: The vcf file has another line that provides the names of all the columns and starts with ##CHROM. From left to right, where are the columns located that actually contain the data with the number of reference and alternate reads for your two samples?**\n",
    "\n",
    "**Answer here**\n",
    "\n",
    "\n",
    "**Question: Look at the first SNP in your sample. What position of the genome is it located at?**\n",
    "\n",
    "**Answer here**\n",
    "\n",
    "**Question: Now keep reading across to the column that starts with ` GT:GQ:SDP...`. This column tells you the order of information that you will read about each sample, separated by a colon ( `:`) between each piece of information. You identified the abbreviations for reference and alternate read counts in a question above. Which positions in this series of information will have your reference and alternate read counts?**\n",
    "\n",
    "**Answer here**\n",
    "\n",
    "**Question: Now keep reading across to your first ZP sample. How many reads were found for the reference and alternate allele? These are the numbers in the positions you identified above.**\n",
    "\n",
    "**Answer here**\n",
    "\n",
    "\n",
    "**Question: the vcf file format automatically assigns a \"genotype\" to each sample. Why does it not make sense to assign genotypes in pooled sequencing?**\n",
    "\n",
    "**Answer here**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f07b3e",
   "metadata": {},
   "source": [
    "19) We ran all of these commands on just a tiny region of the genome to make it run quickly so you could see how each step worked. I ran the same program on the whole genome of all of our samples over the weekend-it took many hours. Using all of the samples together increases the ability to find variants relative to using just two samples because there is so much more information to compare. But, the resulting vcf file has 45 sequencing samples and is quite large, so it is currently stored in a compressed .gz format. To make these easier to work with, you are going to extract just the two samples you are working with from each of these variant files. We will use a program called `bcftools` to do this with a command called `view`. This program will use the list of sample names that you already made (`sample_names.txt`) to extract just those samples from the vcf file and save a smaller file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools view -S sample_names.txt ~/shared_perickso/ZP_all.vcf.gz > subset_snps.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3303a",
   "metadata": {},
   "source": [
    "20) Now confirm that the previous command did what you wanted it to by checking the sample names present in the file using the command below. `query -l` is a shortcut that extracts all the sample names from a vcf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b373241",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools query -l subset_snps.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd8b35",
   "metadata": {},
   "source": [
    "21) We also don't want to include SNPs in regions of the genome that contain repetitive sequences like retrotransposons. These sequences show up in many locations in the genome and often have nearly identical DNA sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e72022",
   "metadata": {},
   "source": [
    "**Question: Why would it be hard to map reads and identify SNPs in repetitive regions?**\n",
    "\n",
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a9fbc",
   "metadata": {},
   "source": [
    "22) I have a file that contains the positions of all of the repetitive sequences in the _Z. indianus_ genome. The file is in a `gff` format, which is basically just a table of chromsomes, start positions, and stop positions for regions of the genome. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aedd52-3eca-49b4-86e2-1b3b81adf87e",
   "metadata": {},
   "source": [
    "**Question: Look at the `zind_repeats.gff` file in ~/`shared_perickso`. Where is the location of the first repetitive element in the file?**\n",
    "\n",
    "**Answere here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956ecfa-4f42-4993-9e6f-5d92a49865de",
   "metadata": {},
   "source": [
    "We can use a program called `bedtools` to remove all of these regions from our vcf file as well. This program essentially does a \"subtraction\" where it will remove any SNPs found within regions defined in the `~/shared_perickso/zind_ref_genome_repeats.gff` file. Run the code below to remove the SNPs in repetitive regions from your vcf. The `\\` after each line indicates that more code is coming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $WD\n",
    "bedtools intersect -v -header \\\n",
    "    -a subset_snps.vcf \\\n",
    "    -b ~/shared_perickso/zind_repeats.gff \\\n",
    "    > subset_snps_no_repeats.vcf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30daba",
   "metadata": {},
   "source": [
    "**Question: Use `wc -l` to count the number of lines in your starting file (`subset_snps.vcf`) and the new file you produced in the command above. How many lines are in each and approximately how many SNPs were removed due to being in repetitive regions?**\n",
    "\n",
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4266d16",
   "metadata": {},
   "source": [
    "23) Some SNPs are just going to be hard to work with. They might have more than two alleles. They may not have very many reads supporting them or they may have way too many reads supporting them, suggesting a potential genome duplication. We want to get rid of those SNPs too. The command below will filter any SNPs that have less than 15 or more than 200 reads supporting them in your dataset. This command will apply several filters and write a new vcf with `.recode` added at the end of the file name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcftools --vcf subset_snps_no_repeats.vcf \\\n",
    "--min-meanDP 15 \\\n",
    "--max-meanDP 200 \\\n",
    "--min-alleles 2 \\\n",
    "--max-alleles 2 \\\n",
    "--max-missing 0.5 \\\n",
    "--recode \\\n",
    "--out subset_snps_no_repeats_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7005d",
   "metadata": {},
   "source": [
    "**Question: At the end of running this command, it will tell you how many SNPs were retained. How many were removed due to this filtering?**\n",
    "\n",
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b5451",
   "metadata": {},
   "source": [
    "24) Now that we have a cleaned up file, let's calculate how many reads we have for each SNP in each sample on average using the `--depth` option of the program `vcftools`. Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcftools --vcf subset_snps_no_repeats_filtered.recode.vcf --depth --out subset_snps_no_repeats_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c94fc7",
   "metadata": {},
   "source": [
    "25) The above command created a new file called `subset_snps_no_repeats_filtered.idepth` that contains the depth, averaged across all SNPs in the file (column is named `MEAN_DEPTH`)\n",
    "\n",
    "**Question: Look at this file and report the mean sequencing depth of your two samples, then explain in your own words what this number means.**\n",
    "\n",
    "**Answer here**\n",
    "\n",
    "**Question: how does this number compare to the sequencing depth that you calculated in your original bam file near the beginning of class?**\n",
    "\n",
    "**Answer here** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc7bbf",
   "metadata": {},
   "source": [
    "26) We are going to stop here today with our filtered vcf file which we will use to analyze genetic variation after spring break, but before we go, we'll do a mini writing assignment:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c41b45",
   "metadata": {},
   "source": [
    "**Question: Write a one-paragraph methods summary that you might include in a paper if you were writing this work from today up for a paper. Your paragraph should describe the important steps that were done and the software that was used.  You do not need to describe the steps that were just checking on things like the sequencing depth or examining the gff file. But you should describe the SNP identification and filterning steps.**\n",
    "\n",
    "**Answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44c113",
   "metadata": {},
   "source": [
    "### check to make sure you have answered every question and have a complete methods section before submitting! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474bff5",
   "metadata": {},
   "source": [
    "When you are done, you can print this as a PDF then upload to the Blackboard assignment for this week's lab. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econdata2",
   "language": "python",
   "name": "econdata2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
